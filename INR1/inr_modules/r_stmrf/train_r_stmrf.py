"""
R-STMRF è®­ç»ƒè„šæœ¬

å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - æ•°æ®åŠ è½½ï¼ˆä¿ç•™ TimeBinSamplerï¼‰
    - æ¨¡å‹åˆå§‹åŒ–
    - ç‰©ç†çº¦æŸæŸå¤±
    - è®­ç»ƒå’ŒéªŒè¯å¾ªç¯
    - æ¨¡å‹ä¿å­˜
"""

import os
import sys
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, random_split, Subset
from tqdm import tqdm

# æ·»åŠ æ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

from .config_r_stmrf import get_config_r_stmrf, print_config_r_stmrf
from .r_stmrf_model import R_STMRF_Model
from .physics_losses_r_stmrf import combined_physics_loss
from .sliding_dataset import SlidingWindowBatchProcessor
from .tec_gradient_bank import TecGradientBank

from data_managers import SpaceWeatherManager, TECDataManager, IRINeuralProxy
from data_managers.FY_dataloader import FY3D_Dataset, TimeBinSampler


class SubsetTimeBinSampler(TimeBinSampler):
    """
    TimeBinSampler çš„ Subset ç‰ˆæœ¬

    ç”¨äºåœ¨ random_split ä¹‹åä»ç„¶ä½¿ç”¨æ—¶é—´åˆ†ç®±ç­–ç•¥

    å…³é”®ä¿®å¤ï¼šå°†åŸå§‹æ•°æ®é›†çš„ç»å¯¹ç´¢å¼•è½¬æ¢ä¸º Subset çš„ç›¸å¯¹ç´¢å¼•
    """
    def __init__(self, subset: Subset, batch_size: int, shuffle: bool = True, drop_last: bool = False):
        # è·å–åº•å±‚çš„ FY3D_Dataset
        base_dataset = subset.dataset
        subset_indices = subset.indices

        # åˆ›å»ºä»åŸå§‹ç´¢å¼•åˆ° Subset ç›¸å¯¹ç´¢å¼•çš„æ˜ å°„
        # ä¾‹å¦‚ï¼šsubset.indices = [100, 200, 300] -> {100: 0, 200: 1, 300: 2}
        original_to_subset_idx = {orig_idx: subset_idx
                                  for subset_idx, orig_idx in enumerate(subset_indices)}

        # æ„å»º Subset çš„ indices_by_bin
        # åªä¿ç•™åœ¨ subset ä¸­çš„ç´¢å¼•ï¼Œå¹¶è½¬æ¢ä¸ºç›¸å¯¹ç´¢å¼•
        filtered_indices_by_bin = {}

        for bin_id, indices in base_dataset.indices_by_bin.items():
            # è¿‡æ»¤å‡ºåœ¨ subset ä¸­çš„ç´¢å¼•ï¼Œå¹¶è½¬æ¢ä¸º Subset çš„ç›¸å¯¹ç´¢å¼•
            filtered_indices = []
            for idx in indices:
                if idx in original_to_subset_idx:
                    filtered_indices.append(original_to_subset_idx[idx])

            if len(filtered_indices) > 0:
                filtered_indices_by_bin[bin_id] = np.array(filtered_indices)

        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ dataset å¯¹è±¡ï¼Œåªç”¨äºå­˜å‚¨ indices_by_bin
        class TempDataset:
            def __init__(self, indices_by_bin):
                self.indices_by_bin = indices_by_bin

        temp_dataset = TempDataset(filtered_indices_by_bin)

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆä½†ä½¿ç”¨ä¸´æ—¶ datasetï¼‰
        self.dataset = temp_dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.drop_last = drop_last


def train_one_epoch(model, train_loader, batch_processor, gradient_bank, optimizer, device, config, epoch, scaler=None):
    """
    è®­ç»ƒä¸€ä¸ª epoch

    Args:
        model: R-STMRF æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ® loader
        batch_processor: æ‰¹æ¬¡å¤„ç†å™¨
        gradient_bank: TecGradientBank å®ä¾‹ï¼ˆé¢„è®¡ç®—æ¢¯åº¦åº“ï¼‰
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        config: é…ç½®å­—å…¸
        epoch: å½“å‰ epochï¼ˆä» 0 å¼€å§‹ï¼‰
        scaler: GradScaler for AMP (è‡ªåŠ¨æ··åˆç²¾åº¦)

    Returns:
        avg_loss: å¹³å‡æŸå¤±
        loss_dict: å„é¡¹æŸå¤±çš„è¯¦ç»†å­—å…¸
            - mse: çº¯é¢„æµ‹è¯¯å·®ï¼ˆä¸å—ä¸ç¡®å®šæ€§å½±å“ï¼‰
            - nll: å¼‚æ–¹å·®æŸå¤±ï¼ˆå¯èƒ½ä¸ºè´Ÿï¼‰
            - total: æ€»ä¼˜åŒ–ç›®æ ‡
            - chapman, tec_direction: ç‰©ç†æŸå¤±åˆ†é¡¹
            - physics_total: ç‰©ç†æŸå¤±æ€»å’Œ
    """
    model.train()

    # ä¸ç¡®å®šæ€§ Warm-up é€»è¾‘
    warmup_epochs = config.get('uncertainty_warmup_epochs', 5)
    use_uncertainty = config.get('use_uncertainty', True) and (epoch >= warmup_epochs)

    if epoch < warmup_epochs and config.get('use_uncertainty', True):
        print(f"  [Warm-up] Epoch {epoch+1}/{warmup_epochs}: ä½¿ç”¨ MSE+ç‰©ç†æŸå¤±ï¼ˆå…³é—­ä¸ç¡®å®šæ€§ï¼‰")

    # ç»Ÿè®¡å˜é‡
    total_loss = 0.0
    total_mse = 0.0  # çº¯ MSEï¼ˆå§‹ç»ˆè®¡ç®—ï¼‰
    total_nll = 0.0  # NLL æŸå¤±ï¼ˆå¯èƒ½ä¸ºè´Ÿï¼‰
    total_physics = 0.0
    total_chapman = 0.0
    total_tec_direction = 0.0
    num_batches = 0

    use_amp = config.get('use_amp', False) and scaler is not None

    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]", leave=False)

    for batch_idx, batch_data in enumerate(pbar):
        # 1. å¤„ç†æ‰¹æ¬¡æ•°æ®ï¼ˆè·å–åºåˆ—ï¼‰
        coords, target_ne, sw_seq = batch_processor.process_batch(batch_data)

        # 2. æŸ¥è¯¢é¢„è®¡ç®—çš„ TEC æ¢¯åº¦æ–¹å‘ï¼ˆå¿«é€ŸæŸ¥è¯¢ï¼Œæ—  ConvLSTM è®¡ç®—ï¼‰
        timestamps = coords[:, 3]  # æå–æ—¶é—´ç»´åº¦ [Batch]
        tec_grad_direction = gradient_bank.get_interpolated_gradient(timestamps)  # [Batch, 2, H, W]

        # åˆ¤æ–­æ˜¯å¦éœ€è¦è®¡ç®—ç‰©ç†æŸå¤±ï¼ˆé—´æ­‡æ€§è®¡ç®—ä»¥åŠ é€Ÿè®­ç»ƒï¼‰
        physics_loss_freq = config.get('physics_loss_freq', 10)  # é»˜è®¤æ¯10ä¸ªbatchè®¡ç®—ä¸€æ¬¡
        compute_physics = (batch_idx % physics_loss_freq == 0)

        # åªåœ¨éœ€è¦ç‰©ç†æŸå¤±æ—¶å¯ç”¨æ¢¯åº¦
        if compute_physics:
            coords.requires_grad_(True)

        # 3. å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒAMPï¼‰
        with torch.amp.autocast('cuda', enabled=use_amp):
            pred_ne, log_var, correction, extras = model(coords, sw_seq, tec_grad_direction)

            # ==================== 3.1 å§‹ç»ˆè®¡ç®—çº¯ MSEï¼ˆç”¨äºç›‘æ§ç²¾åº¦ï¼‰====================
            pure_mse = F.mse_loss(pred_ne, target_ne)

            # ==================== 3.2 è®¡ç®—ä¸»æŸå¤±ï¼ˆæ ¹æ® warm-up çŠ¶æ€é€‰æ‹©ï¼‰====================
            if use_uncertainty:
                # Warm-up ç»“æŸåï¼šä½¿ç”¨å¼‚æ–¹å·®æŸå¤±ï¼ˆNLLï¼‰
                # 1. çº¦æŸ log_var èŒƒå›´ï¼ˆé˜²æ­¢å´©å¡Œï¼‰
                log_var_clamped = torch.clamp(
                    log_var,
                    min=config.get('log_var_min', -10.0),
                    max=config.get('log_var_max', 10.0)
                )

                # 2. è®¡ç®— NLLï¼ˆå¯èƒ½ä¸ºè´Ÿï¼‰
                precision = torch.exp(-log_var_clamped)
                mse_term = (pred_ne - target_ne) ** 2
                nll_loss = torch.mean(0.5 * precision * mse_term + 0.5 * log_var_clamped)

                # 3. æ·»åŠ  log_var æ­£åˆ™åŒ–ï¼ˆæƒ©ç½šæç«¯æ–¹å·®ï¼Œé¼“åŠ±æ¥è¿‘ 1ï¼‰
                log_var_reg = config.get('log_var_regularization', 0.001)
                log_var_penalty = log_var_reg * (log_var_clamped ** 2).mean()

                loss_main = nll_loss + log_var_penalty
            else:
                # Warm-up æœŸé—´æˆ–é…ç½®å…³é—­ï¼šä½¿ç”¨çº¯ MSE
                loss_main = pure_mse
                nll_loss = pure_mse  # ç”¨äºè®°å½•ï¼ˆæ­¤æ—¶ NLL = MSEï¼‰

        # 4. è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±ï¼ˆé—´æ­‡æ€§è®¡ç®—ï¼‰
        # âš ï¸ å…³é”®ï¼šç‰©ç†æŸå¤±åŒ…å«äºŒé˜¶å¯¼æ•°ï¼Œå¿…é¡»åœ¨AMPå¤–è®¡ç®—ï¼ˆå·²åœ¨physics_lossesä¸­ç¦ç”¨AMPï¼‰
        if compute_physics:
            # ç‰©ç†æŸå¤±è®¡ç®—å·²åœ¨å‡½æ•°å†…éƒ¨ç¦ç”¨AMPï¼ˆè§chapman_smoothness_lossï¼‰
            loss_physics, physics_dict = combined_physics_loss(
                pred_ne=pred_ne,
                coords=coords,
                tec_grad_direction=extras.get('tec_grad_direction'),
                coords_normalized=extras.get('coords_normalized'),
                w_chapman=config['w_chapman'],
                w_tec_direction=config.get('w_tec_direction', 0.05),
                tec_lat_range=config['lat_range'],
                tec_lon_range=config['lon_range']
            )
        else:
            # è·³è¿‡ç‰©ç†æŸå¤±è®¡ç®—ï¼Œä½¿ç”¨é›¶æŸå¤±
            loss_physics = 0.0
            physics_dict = {
                'physics_total': 0.0,
                'chapman': 0.0,
                'tec_direction': 0.0
            }

        # 5. æ€»æŸå¤±
        loss = config['w_mse'] * loss_main + loss_physics

        # 6. åå‘ä¼ æ’­ï¼ˆæ”¯æŒAMPï¼‰
        optimizer.zero_grad()

        if use_amp:
            # AMPåå‘ä¼ æ’­
            scaler.scale(loss).backward()

            # æ¢¯åº¦è£å‰ª
            if config['grad_clip'] is not None:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])

            scaler.step(optimizer)
            scaler.update()
        else:
            # æ ‡å‡†åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            if config['grad_clip'] is not None:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])

            optimizer.step()

        # 7. ç»Ÿè®¡
        total_loss += loss.item()
        total_mse += pure_mse.item()  # çº¯ MSE
        total_nll += nll_loss.item() if use_uncertainty else pure_mse.item()  # NLL
        total_physics += physics_dict['physics_total']
        total_chapman += physics_dict['chapman']
        total_tec_direction += physics_dict.get('tec_direction', 0.0)
        num_batches += 1

        # æ›´æ–°è¿›åº¦æ¡
        physics_str = f"{physics_dict['physics_total']:.4f}" if compute_physics else "skip"
        uncertainty_str = "NLL" if use_uncertainty else "MSE"
        pbar.set_postfix({
            'Loss': f"{loss.item():.4f}",
            'Pure_MSE': f"{pure_mse.item():.4f}",
            'Mode': uncertainty_str,
            'Physics': physics_str
        })

    # å¹³å‡æŸå¤±
    avg_loss = total_loss / num_batches
    loss_dict = {
        'total': avg_loss,                          # æ€»ä¼˜åŒ–ç›®æ ‡
        'mse': total_mse / num_batches,             # çº¯ MSEï¼ˆç²¾åº¦ç›‘æ§ï¼‰
        'nll': total_nll / num_batches,             # NLL æŸå¤±ï¼ˆå¯èƒ½ä¸ºè´Ÿï¼‰
        'physics_total': total_physics / num_batches,  # ç‰©ç†æŸå¤±æ€»å’Œ
        'chapman': total_chapman / num_batches,        # Chapman å¹³æ»‘
        'tec_direction': total_tec_direction / num_batches  # TEC æ–¹å‘
    }

    return avg_loss, loss_dict


def validate(model, val_loader, batch_processor, gradient_bank, device, config):
    """
    éªŒè¯æ¨¡å‹

    Args:
        model: R-STMRF æ¨¡å‹
        val_loader: éªŒè¯æ•°æ® loader
        batch_processor: æ‰¹æ¬¡å¤„ç†å™¨
        gradient_bank: TecGradientBank å®ä¾‹ï¼ˆé¢„è®¡ç®—æ¢¯åº¦åº“ï¼‰
        device: è®¾å¤‡
        config: é…ç½®å­—å…¸

    Returns:
        avg_loss: å¹³å‡æŸå¤±
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    model.eval()

    total_loss = 0.0
    total_mse = 0.0
    num_batches = 0

    all_preds = []
    all_targets = []

    with torch.no_grad():
        for batch_data in tqdm(val_loader, desc="Validating", leave=False):
            # å¤„ç†æ‰¹æ¬¡æ•°æ®
            coords, target_ne, sw_seq = batch_processor.process_batch(batch_data)

            # æŸ¥è¯¢é¢„è®¡ç®—çš„ TEC æ¢¯åº¦æ–¹å‘
            timestamps = coords[:, 3]  # æå–æ—¶é—´ç»´åº¦ [Batch]
            tec_grad_direction = gradient_bank.get_interpolated_gradient(timestamps)  # [Batch, 2, H, W]

            # å‰å‘ä¼ æ’­
            pred_ne, log_var, correction, extras = model(coords, sw_seq, tec_grad_direction)

            # MSE æŸå¤±
            loss_mse = F.mse_loss(pred_ne, target_ne)

            total_loss += loss_mse.item()
            total_mse += loss_mse.item()
            num_batches += 1

            # æ”¶é›†é¢„æµ‹å’ŒçœŸå€¼ï¼ˆç”¨äºè®¡ç®—æŒ‡æ ‡ï¼‰
            all_preds.append(pred_ne.cpu())
            all_targets.append(target_ne.cpu())

    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / num_batches

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    all_preds = torch.cat(all_preds, dim=0).numpy()
    all_targets = torch.cat(all_targets, dim=0).numpy()

    mae = np.mean(np.abs(all_preds - all_targets))
    rmse = np.sqrt(np.mean((all_preds - all_targets) ** 2))
    r2 = 1 - np.sum((all_preds - all_targets) ** 2) / np.sum((all_targets - np.mean(all_targets)) ** 2)

    metrics = {
        'loss': avg_loss,
        'mse': total_mse / num_batches,
        'mae': mae,
        'rmse': rmse,
        'r2': r2
    }

    return avg_loss, metrics


def train_r_stmrf(config):
    """
    ä¸»è®­ç»ƒå‡½æ•°

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        ç›¸å…³ç®¡ç†å™¨
    """
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config['seed'])
    np.random.seed(config['seed'])

    device = torch.device(config['device'])
    print(f"\n{'='*70}")
    print(f"R-STMRF è®­ç»ƒæµç¨‹")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"{'='*70}\n")

    # ==================== 1. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨ ====================
    print("[æ­¥éª¤ 1] åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")

    sw_manager = SpaceWeatherManager(
        txt_path=config['sw_path'],
        start_date_str=config['start_date_str'],
        total_hours=config['total_hours'],
        seq_len=config['seq_len'],
        device=device
    )

    tec_manager = TECDataManager(
        tec_map_path=config['tec_path'],
        total_hours=config['total_hours'],
        seq_len=config['seq_len'],
        device=device
        # ä¿æŒåŸå§‹åˆ†è¾¨ç‡73Ã—73ï¼Œä¸å†ä½¿ç”¨é™é‡‡æ ·
    )

    # ==================== 2. åŠ è½½ IRI ç¥ç»ä»£ç† ====================
    print("\n[æ­¥éª¤ 2] åŠ è½½ IRI ç¥ç»ä»£ç†åœº...")

    if not os.path.exists(config['iri_proxy_path']):
        raise FileNotFoundError(f"IRI ä»£ç†æœªæ‰¾åˆ°: {config['iri_proxy_path']}")

    iri_proxy = IRINeuralProxy(layers=[4, 128, 128, 128, 128, 1]).to(device)
    state_dict = torch.load(config['iri_proxy_path'], map_location=device)
    iri_proxy.load_state_dict(state_dict)
    iri_proxy.eval()
    print("  âœ“ IRI ä»£ç†å·²åŠ è½½å¹¶å†»ç»“")

    # ==================== 3. å‡†å¤‡æ•°æ®é›†ï¼ˆéšæœºåˆ’åˆ†ï¼‰====================
    print("\n[æ­¥éª¤ 3] å‡†å¤‡æ•°æ®é›†ï¼ˆéšæœºåˆ’åˆ†ï¼‰...")

    # å†…å­˜æ˜ å°„è®¾ç½®
    use_memmap = config.get('use_memmap', False)
    if use_memmap:
        print(f"  âœ“ å¯ç”¨Memory-MappedåŠ è½½ - æŒ‰éœ€ä»ç£ç›˜è¯»å–ï¼Œå¤§å¹…å‡å°‘å†…å­˜å ç”¨")
    else:
        print(f"  ä½¿ç”¨å…¨é‡åŠ è½½æ¨¡å¼ - ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ï¼ˆæ›´å¿«ä½†å†…å­˜å ç”¨å¤§ï¼‰")

    # åŠ è½½å…¨éƒ¨æ•°æ®
    full_dataset = FY3D_Dataset(
        npy_path=config['fy_path'],
        mode='train',
        val_days=[],  # ä¸ä½¿ç”¨æ—¥æœŸè¿‡æ»¤ï¼ŒåŠ è½½å…¨éƒ¨æ•°æ®
        bin_size_hours=config['bin_size_hours'],
        use_memmap=use_memmap
    )

    # éšæœºåˆ’åˆ†
    total_samples = len(full_dataset)
    val_size = int(total_samples * config['val_ratio'])
    train_size = total_samples - val_size

    print(f"  æ€»æ ·æœ¬: {total_samples}")
    print(f"  è®­ç»ƒé›†: {train_size} | éªŒè¯é›†: {val_size}")

    generator = torch.Generator().manual_seed(config['seed'])
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size], generator=generator
    )

    # åˆ›å»ºæ—¶é—´åˆ†ç®± Samplerï¼ˆä¼˜åŒ– ConvLSTM å†…å­˜ä½¿ç”¨ï¼‰
    # é€šè¿‡æ—¶é—´åˆ†ç®±ï¼Œç¡®ä¿æ¯ä¸ª batch å†…çš„æ ·æœ¬æ¥è‡ªç›¸ä¼¼çš„æ—¶é—´çª—å£
    # è¿™æ ·å¯ä»¥æœ€å¤§åŒ–å”¯ä¸€æ—¶é—´çª—å£çš„é‡å¤ç‡ï¼Œå‡å°‘ ConvLSTM çš„é‡å¤è®¡ç®—
    train_sampler = SubsetTimeBinSampler(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        drop_last=False
    )
    val_sampler = SubsetTimeBinSampler(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        drop_last=False
    )

    # åˆ›å»º DataLoaderï¼ˆä½¿ç”¨ batch_samplerï¼‰
    train_loader = DataLoader(
        train_dataset,
        batch_sampler=train_sampler,
        num_workers=config['num_workers'],
        pin_memory=True if device.type == 'cuda' else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_sampler=val_sampler,
        num_workers=config['num_workers'],
        pin_memory=True if device.type == 'cuda' else False
    )

    print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)} | éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
    print(f"  ä½¿ç”¨æ—¶é—´åˆ†ç®±ç­–ç•¥ä¼˜åŒ– ConvLSTM å†…å­˜ä½¿ç”¨")

    # åˆ›å»ºæ‰¹æ¬¡å¤„ç†å™¨
    batch_processor = SlidingWindowBatchProcessor(sw_manager, tec_manager, device)

    # ==================== 4. åˆå§‹åŒ–æ¨¡å‹ ====================
    print("\n[æ­¥éª¤ 4] åˆå§‹åŒ– R-STMRF æ¨¡å‹...")

    model = R_STMRF_Model(
        iri_proxy=iri_proxy,
        lat_range=config['lat_range'],
        lon_range=config['lon_range'],
        alt_range=config['alt_range'],
        sw_manager=sw_manager,
        tec_manager=tec_manager,
        start_date_str=config['start_date_str'],
        config=config
    ).to(device)

    print(f"  æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # ==================== 4.5. åˆå§‹åŒ– TEC æ¢¯åº¦åº“ï¼ˆæ–°æ¶æ„ï¼‰====================
    print("\n[æ­¥éª¤ 4.5] åˆå§‹åŒ– TEC æ¢¯åº¦åº“ï¼ˆç¦»çº¿é¢„è®¡ç®— + å¿«é€ŸæŸ¥è¯¢ï¼‰...")

    gradient_bank_path = config.get('gradient_bank_path')
    if gradient_bank_path is None:
        raise ValueError("é…ç½®ä¸­ç¼ºå°‘ 'gradient_bank_path' å‚æ•°ï¼\n"
                        "è¯·å…ˆè¿è¡Œ precompute_tec_gradient_bank.py ç”Ÿæˆæ¢¯åº¦åº“ï¼Œ"
                        "ç„¶ååœ¨é…ç½®ä¸­æŒ‡å®šè·¯å¾„ã€‚")

    gradient_bank = TecGradientBank(
        gradient_bank_path=gradient_bank_path,
        total_hours=config['total_hours'],
        device=device
    )

    print(f"  âœ“ TEC æ¢¯åº¦åº“åŠ è½½æˆåŠŸ")
    print(f"  âœ“ ä½¿ç”¨å†…å­˜æ˜ å°„ï¼ˆMemory-Mappedï¼‰æ¨¡å¼ï¼ŒRAM å ç”¨ < 10 MB")
    print(f"  âœ“ æ”¯æŒä½™å¼¦å¹³æ–¹æ’å€¼ï¼Œä¿è¯ C1 è¿ç»­æ€§")
    print(f"  âœ“ å®Œå…¨æ¶ˆé™¤ ConvLSTM åœ¨çº¿è®¡ç®—å¼€é”€")

    # ==================== 5. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ====================
    print("\n[æ­¥éª¤ 5] é…ç½®ä¼˜åŒ–å™¨...")

    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['lr'],
        weight_decay=config['weight_decay']
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config['scheduler_type'] == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config['epochs'],
            eta_min=config['min_lr']
        )
    else:
        scheduler = None

    # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
    use_amp = config.get('use_amp', False)
    scaler = None
    if use_amp:
        if device.type == 'cuda':
            scaler = torch.cuda.amp.GradScaler()
            print(f"  âœ“ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰- åŠ é€Ÿè®­ç»ƒå¹¶èŠ‚çœæ˜¾å­˜")
            print(f"  âš ï¸ ChapmanæŸå¤±ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰å°†å¼ºåˆ¶ä½¿ç”¨float32ä»¥é¿å…æ•°å€¼ä¸ç¨³å®š")
        else:
            print(f"  âš ï¸ AMPä»…æ”¯æŒCUDAè®¾å¤‡ï¼Œå·²ç¦ç”¨")
            use_amp = False

    # ==================== 6. è®­ç»ƒå¾ªç¯ ====================
    print("\n[æ­¥éª¤ 6] å¼€å§‹è®­ç»ƒ...")
    physics_freq = config.get('physics_loss_freq', 10)
    if physics_freq > 1:
        print(f"  âš¡ ç‰©ç†æŸå¤±é—´æ­‡æ€§è®¡ç®—ï¼šæ¯ {physics_freq} ä¸ªbatchè®¡ç®—ä¸€æ¬¡ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰")
        print(f"  âš¡ é¢„æœŸåŠ é€Ÿ: ~{physics_freq/2:.1f}Ã— æ¢¯åº¦è®¡ç®—å‡å°‘")
    else:
        print(f"  ğŸ“Š ç‰©ç†æŸå¤±æ¯ä¸ªbatchè®¡ç®—ï¼ˆphysics_loss_freq=1ï¼‰")

    # ä¸ç¡®å®šæ€§ Warm-up æç¤º
    warmup_epochs = config.get('uncertainty_warmup_epochs', 5)
    if config.get('use_uncertainty', True):
        print(f"  ğŸ”¥ ä¸ç¡®å®šæ€§ Warm-up: å‰ {warmup_epochs} ä¸ª epoch ä½¿ç”¨çº¯ MSE")
        print(f"     ä¹‹åå¯ç”¨å¼‚æ–¹å·®æŸå¤±ï¼ˆNLLï¼‰ï¼Œå­¦ä¹ é¢„æµ‹æ–¹å·®")

    print(f"{'='*70}\n")

    # è®­ç»ƒå†å²è®°å½•
    train_losses = []
    val_losses = []
    history = []  # è¯¦ç»†å†å²æ•°æ®ï¼ˆç”¨äºä¸‰è§†å›¾ç»˜å›¾å’Œä¿å­˜ï¼‰
    best_val_loss = float('inf')
    patience_counter = 0

    # å¯¼å…¥ç»˜å›¾å‡½æ•°
    from .plotting import plot_training_curves_3panel

    for epoch in range(config['epochs']):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch+1}/{config['epochs']}")
        print(f"{'='*70}")

        # è®­ç»ƒ
        train_loss, train_dict = train_one_epoch(
            model, train_loader, batch_processor, gradient_bank, optimizer, device, config, epoch, scaler
        )
        train_losses.append(train_loss)

        # éªŒè¯
        val_loss, val_metrics = validate(model, val_loader, batch_processor, gradient_bank, device, config)
        val_losses.append(val_loss)

        # æ‰“å°ç»“æœ
        print(f"\nEpoch {epoch+1} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f}")
        print(f"    - Pure MSE: {train_dict['mse']:.6f}")
        print(f"    - NLL: {train_dict['nll']:.6f}")
        print(f"    - Physics: {train_dict['physics_total']:.6f}")
        print(f"      Â· Chapman: {train_dict['chapman']:.6f}")
        print(f"      Â· TEC Direction: {train_dict['tec_direction']:.6f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.6f}")
        print(f"    - MAE: {val_metrics['mae']:.6f}")
        print(f"    - RMSE: {val_metrics['rmse']:.6f}")
        print(f"    - RÂ²: {val_metrics['r2']:.4f}")

        # æ”¶é›†å†å²æ•°æ®
        history_record = {
            'epoch': epoch + 1,
            'train_mse': train_dict['mse'],
            'val_mse': val_loss,  # éªŒè¯é›†ä½¿ç”¨çº¯ MSE
            'train_nll': train_dict['nll'],
            'total_loss': train_dict['total'],
            'chapman': train_dict['chapman'],
            'tec_direction': train_dict['tec_direction']
        }
        history.append(history_record)

        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None:
            scheduler.step()
            print(f"  å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            save_path = os.path.join(config['save_dir'], 'best_r_stmrf_model.pth')
            torch.save(model.state_dict(), save_path)
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹: {save_path}")
        else:
            patience_counter += 1

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆæ¯ä¸ª epoch æ›´æ–°ï¼‰
        # è¿™æ ·å¯ä»¥å®æ—¶ç›‘æ§è®­ç»ƒçŠ¶æ€
        plot_training_curves_3panel(
            history,
            save_path=os.path.join(config['save_dir'], 'training_curves_3panel.png')
        )

        # æ—©åœ
        if config['early_stopping'] and patience_counter >= config['patience']:
            print(f"\næ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±è¿ç»­ {config['patience']} è½®æœªæ”¹å–„")
            break

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % config['save_interval'] == 0:
            save_path = os.path.join(config['save_dir'], f'r_stmrf_epoch_{epoch+1}.pth')
            torch.save(model.state_dict(), save_path)

    print(f"\n{'='*70}")
    print("è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"{'='*70}\n")

    # ==================== 7. ä¿å­˜è®­ç»ƒå†å² ====================
    import json
    history_path = os.path.join(config['save_dir'], 'training_history.json')
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    print(f"âœ“ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

    # æœ€ç»ˆç»˜å›¾
    print(f"âœ“ æœ€ç»ˆè®­ç»ƒæ›²çº¿: {os.path.join(config['save_dir'], 'training_curves_3panel.png')}\n")

    return model, train_losses, val_losses, train_loader, val_loader, sw_manager, tec_manager, gradient_bank, batch_processor


# ======================== ä¸»å‡½æ•° ========================
if __name__ == '__main__':
    # è·å–é…ç½®
    config = get_config_r_stmrf()
    print_config_r_stmrf()

    # å¼€å§‹è®­ç»ƒ
    model, train_losses, val_losses, train_loader, val_loader, sw_manager, tec_manager = train_r_stmrf(config)

    print("\nè®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼")
