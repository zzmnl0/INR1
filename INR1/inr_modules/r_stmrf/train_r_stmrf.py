"""
R-STMRF è®­ç»ƒè„šæœ¬

å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - æ•°æ®åŠ è½½ï¼ˆä¿ç•™ TimeBinSamplerï¼‰
    - æ¨¡å‹åˆå§‹åŒ–
    - ç‰©ç†çº¦æŸæŸå¤±
    - è®­ç»ƒå’ŒéªŒè¯å¾ªç¯
    - æ¨¡å‹ä¿å­˜
"""

import os
import sys
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, random_split, Subset
from tqdm import tqdm

# æ·»åŠ æ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

from config_r_stmrf import get_config_r_stmrf, print_config_r_stmrf
from r_stmrf_model import R_STMRF_Model
from physics_losses_r_stmrf import combined_physics_loss
from sliding_dataset import SlidingWindowBatchProcessor

from data_managers import SpaceWeatherManager, TECDataManager, IRINeuralProxy
from data_managers.FY_dataloader import FY3D_Dataset, TimeBinSampler


class SubsetTimeBinSampler(TimeBinSampler):
    """
    TimeBinSampler çš„ Subset ç‰ˆæœ¬

    ç”¨äºåœ¨ random_split ä¹‹åä»ç„¶ä½¿ç”¨æ—¶é—´åˆ†ç®±ç­–ç•¥
    """
    def __init__(self, subset: Subset, batch_size: int, shuffle: bool = True, drop_last: bool = False):
        # è·å–åº•å±‚çš„ FY3D_Dataset
        base_dataset = subset.dataset
        subset_indices = subset.indices

        # æ„å»º Subset çš„ indices_by_bin
        # åªä¿ç•™åœ¨ subset ä¸­çš„ç´¢å¼•
        subset_indices_set = set(subset_indices)
        filtered_indices_by_bin = {}

        for bin_id, indices in base_dataset.indices_by_bin.items():
            # è¿‡æ»¤å‡ºåœ¨ subset ä¸­çš„ç´¢å¼•
            filtered_indices = np.array([idx for idx in indices if idx in subset_indices_set])
            if len(filtered_indices) > 0:
                filtered_indices_by_bin[bin_id] = filtered_indices

        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ dataset å¯¹è±¡ï¼Œåªç”¨äºå­˜å‚¨ indices_by_bin
        class TempDataset:
            def __init__(self, indices_by_bin):
                self.indices_by_bin = indices_by_bin

        temp_dataset = TempDataset(filtered_indices_by_bin)

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆä½†ä½¿ç”¨ä¸´æ—¶ datasetï¼‰
        self.dataset = temp_dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.drop_last = drop_last


def train_one_epoch(model, train_loader, batch_processor, optimizer, device, config, epoch, use_tec_cache=False):
    """
    è®­ç»ƒä¸€ä¸ª epoch

    Args:
        model: R-STMRF æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ® loader
        batch_processor: æ‰¹æ¬¡å¤„ç†å™¨
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        config: é…ç½®å­—å…¸
        epoch: å½“å‰ epoch
        use_tec_cache: æ˜¯å¦ä½¿ç”¨TECç¼“å­˜ï¼ˆå¤šæ—¶é—´å°ºåº¦ä¼˜åŒ–ï¼‰

    Returns:
        avg_loss: å¹³å‡æŸå¤±
        loss_dict: å„é¡¹æŸå¤±çš„å­—å…¸
    """
    model.train()

    total_loss = 0.0
    total_mse = 0.0
    total_physics = 0.0
    total_chapman = 0.0
    total_tec_direction = 0.0
    num_batches = 0

    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]", leave=False)

    for batch_idx, batch_data in enumerate(pbar):
        # 1. å¤„ç†æ‰¹æ¬¡æ•°æ®ï¼ˆè·å–åºåˆ—ï¼Œè¯†åˆ«å”¯ä¸€æ—¶é—´çª—å£ï¼‰
        coords, target_ne, sw_seq, unique_tec_map_seq, tec_indices, target_tec_map = batch_processor.process_batch(batch_data)

        # åˆ¤æ–­æ˜¯å¦éœ€è¦è®¡ç®—ç‰©ç†æŸå¤±ï¼ˆé—´æ­‡æ€§è®¡ç®—ä»¥åŠ é€Ÿè®­ç»ƒï¼‰
        physics_loss_freq = config.get('physics_loss_freq', 10)  # é»˜è®¤æ¯10ä¸ªbatchè®¡ç®—ä¸€æ¬¡
        compute_physics = (batch_idx % physics_loss_freq == 0)

        # åªåœ¨éœ€è¦ç‰©ç†æŸå¤±æ—¶å¯ç”¨æ¢¯åº¦
        if compute_physics:
            coords.requires_grad_(True)

        # 2. å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼šç¼“å­˜ä¼˜åŒ– / å‘åå…¼å®¹ï¼‰
        if use_tec_cache:
            # å¤šæ—¶é—´å°ºåº¦ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜ï¼ŒConvLSTMåªåœ¨æ•´ç‚¹å°æ—¶è¿è¡Œ
            pred_ne, log_var, correction, extras = model(coords, sw_seq)
        else:
            # å‘åå…¼å®¹ï¼šç›´æ¥å¤„ç†å”¯ä¸€çª—å£ï¼ˆConvLSTMä¸éšbatchå¢é•¿ï¼‰
            pred_ne, log_var, correction, extras = model(coords, sw_seq, unique_tec_map_seq, tec_indices)

        # 3. è®¡ç®—ä¸»æŸå¤±ï¼ˆMSE æˆ– Huberï¼‰
        if config['use_uncertainty']:
            # å¼‚æ–¹å·®æŸå¤±
            precision = torch.exp(-log_var)
            mse_term = (pred_ne - target_ne) ** 2
            loss_main = torch.mean(0.5 * precision * mse_term + 0.5 * log_var)
        else:
            # ç®€å• MSE
            loss_main = F.mse_loss(pred_ne, target_ne)

        # 4. è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±ï¼ˆé—´æ­‡æ€§è®¡ç®—ï¼‰
        if compute_physics:
            loss_physics, physics_dict = combined_physics_loss(
                pred_ne=pred_ne,
                coords=coords,
                tec_grad_direction=extras.get('tec_grad_direction'),  # æ–°è®¾è®¡
                coords_normalized=extras.get('coords_normalized'),  # æ–°è®¾è®¡
                w_chapman=config['w_chapman'],
                w_tec_direction=config.get('w_tec_direction', 0.05),  # æ–°è®¾è®¡ - æ¢¯åº¦æ–¹å‘æƒé‡
                target_tec_map=target_tec_map,  # å…¼å®¹æ—§è®¾è®¡
                w_tec_align=config.get('w_tec_align', 0.0),  # æ—§è®¾è®¡å·²å¼ƒç”¨ï¼Œè®¾ä¸º 0
                tec_lat_range=config['lat_range'],
                tec_lon_range=config['lon_range']
            )
        else:
            # è·³è¿‡ç‰©ç†æŸå¤±è®¡ç®—ï¼Œä½¿ç”¨é›¶æŸå¤±
            loss_physics = 0.0
            physics_dict = {
                'physics_total': 0.0,
                'chapman': 0.0,
                'tec_direction': 0.0
            }

        # 5. æ€»æŸå¤±
        loss = config['w_mse'] * loss_main + loss_physics

        # 6. åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        if config['grad_clip'] is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])

        optimizer.step()

        # 7. ç»Ÿè®¡
        total_loss += loss.item()
        total_mse += loss_main.item()
        total_physics += physics_dict['physics_total']
        total_chapman += physics_dict['chapman']
        total_tec_direction += physics_dict.get('tec_direction', 0.0)
        num_batches += 1

        # æ›´æ–°è¿›åº¦æ¡
        physics_str = f"{physics_dict['physics_total']:.4f}" if compute_physics else "skip"
        pbar.set_postfix({
            'Loss': f"{loss.item():.4f}",
            'MSE': f"{loss_main.item():.4f}",
            'Physics': physics_str
        })

    # å¹³å‡æŸå¤±
    avg_loss = total_loss / num_batches
    loss_dict = {
        'total': avg_loss,
        'mse': total_mse / num_batches,
        'physics': total_physics / num_batches,
        'chapman': total_chapman / num_batches,
        'tec_direction': total_tec_direction / num_batches
    }

    return avg_loss, loss_dict


def validate(model, val_loader, batch_processor, device, config, use_tec_cache=False):
    """
    éªŒè¯æ¨¡å‹

    Args:
        model: R-STMRF æ¨¡å‹
        val_loader: éªŒè¯æ•°æ® loader
        batch_processor: æ‰¹æ¬¡å¤„ç†å™¨
        device: è®¾å¤‡
        config: é…ç½®å­—å…¸
        use_tec_cache: æ˜¯å¦ä½¿ç”¨TECç¼“å­˜ï¼ˆå¤šæ—¶é—´å°ºåº¦ä¼˜åŒ–ï¼‰

    Returns:
        avg_loss: å¹³å‡æŸå¤±
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    model.eval()

    total_loss = 0.0
    total_mse = 0.0
    num_batches = 0

    all_preds = []
    all_targets = []

    with torch.no_grad():
        for batch_data in tqdm(val_loader, desc="Validating", leave=False):
            # å¤„ç†æ‰¹æ¬¡ï¼ˆè¯†åˆ«å”¯ä¸€æ—¶é—´çª—å£ï¼‰
            coords, target_ne, sw_seq, unique_tec_map_seq, tec_indices, target_tec_map = batch_processor.process_batch(batch_data)

            # å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼‰
            if use_tec_cache:
                pred_ne, log_var, correction, extras = model(coords, sw_seq)
            else:
                pred_ne, log_var, correction, extras = model(coords, sw_seq, unique_tec_map_seq, tec_indices)

            # MSE æŸå¤±
            loss_mse = F.mse_loss(pred_ne, target_ne)

            total_loss += loss_mse.item()
            total_mse += loss_mse.item()
            num_batches += 1

            # æ”¶é›†é¢„æµ‹å’ŒçœŸå€¼ï¼ˆç”¨äºè®¡ç®—æŒ‡æ ‡ï¼‰
            all_preds.append(pred_ne.cpu())
            all_targets.append(target_ne.cpu())

    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / num_batches

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    all_preds = torch.cat(all_preds, dim=0).numpy()
    all_targets = torch.cat(all_targets, dim=0).numpy()

    mae = np.mean(np.abs(all_preds - all_targets))
    rmse = np.sqrt(np.mean((all_preds - all_targets) ** 2))
    r2 = 1 - np.sum((all_preds - all_targets) ** 2) / np.sum((all_targets - np.mean(all_targets)) ** 2)

    metrics = {
        'loss': avg_loss,
        'mse': total_mse / num_batches,
        'mae': mae,
        'rmse': rmse,
        'r2': r2
    }

    return avg_loss, metrics


def train_r_stmrf(config):
    """
    ä¸»è®­ç»ƒå‡½æ•°

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        ç›¸å…³ç®¡ç†å™¨
    """
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config['seed'])
    np.random.seed(config['seed'])

    device = torch.device(config['device'])
    print(f"\n{'='*70}")
    print(f"R-STMRF è®­ç»ƒæµç¨‹")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"{'='*70}\n")

    # ==================== 1. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨ ====================
    print("[æ­¥éª¤ 1] åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")

    sw_manager = SpaceWeatherManager(
        txt_path=config['sw_path'],
        start_date_str=config['start_date_str'],
        total_hours=config['total_hours'],
        seq_len=config['seq_len'],
        device=device
    )

    tec_manager = TECDataManager(
        tec_map_path=config['tec_path'],
        total_hours=config['total_hours'],
        seq_len=config['seq_len'],
        device=device
        # ä¿æŒåŸå§‹åˆ†è¾¨ç‡73Ã—73ï¼Œä¸å†ä½¿ç”¨é™é‡‡æ ·
    )

    # ==================== 2. åŠ è½½ IRI ç¥ç»ä»£ç† ====================
    print("\n[æ­¥éª¤ 2] åŠ è½½ IRI ç¥ç»ä»£ç†åœº...")

    if not os.path.exists(config['iri_proxy_path']):
        raise FileNotFoundError(f"IRI ä»£ç†æœªæ‰¾åˆ°: {config['iri_proxy_path']}")

    iri_proxy = IRINeuralProxy(layers=[4, 128, 128, 128, 128, 1]).to(device)
    state_dict = torch.load(config['iri_proxy_path'], map_location=device)
    iri_proxy.load_state_dict(state_dict)
    iri_proxy.eval()
    print("  âœ“ IRI ä»£ç†å·²åŠ è½½å¹¶å†»ç»“")

    # ==================== 3. å‡†å¤‡æ•°æ®é›†ï¼ˆéšæœºåˆ’åˆ†ï¼‰====================
    print("\n[æ­¥éª¤ 3] å‡†å¤‡æ•°æ®é›†ï¼ˆéšæœºåˆ’åˆ†ï¼‰...")

    # åŠ è½½å…¨éƒ¨æ•°æ®
    full_dataset = FY3D_Dataset(
        npy_path=config['fy_path'],
        mode='train',
        val_days=[],  # ä¸ä½¿ç”¨æ—¥æœŸè¿‡æ»¤ï¼ŒåŠ è½½å…¨éƒ¨æ•°æ®
        bin_size_hours=config['bin_size_hours']
    )

    # éšæœºåˆ’åˆ†
    total_samples = len(full_dataset)
    val_size = int(total_samples * config['val_ratio'])
    train_size = total_samples - val_size

    print(f"  æ€»æ ·æœ¬: {total_samples}")
    print(f"  è®­ç»ƒé›†: {train_size} | éªŒè¯é›†: {val_size}")

    generator = torch.Generator().manual_seed(config['seed'])
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size], generator=generator
    )

    # åˆ›å»ºæ—¶é—´åˆ†ç®± Samplerï¼ˆä¼˜åŒ– ConvLSTM å†…å­˜ä½¿ç”¨ï¼‰
    # é€šè¿‡æ—¶é—´åˆ†ç®±ï¼Œç¡®ä¿æ¯ä¸ª batch å†…çš„æ ·æœ¬æ¥è‡ªç›¸ä¼¼çš„æ—¶é—´çª—å£
    # è¿™æ ·å¯ä»¥æœ€å¤§åŒ–å”¯ä¸€æ—¶é—´çª—å£çš„é‡å¤ç‡ï¼Œå‡å°‘ ConvLSTM çš„é‡å¤è®¡ç®—
    train_sampler = SubsetTimeBinSampler(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        drop_last=False
    )
    val_sampler = SubsetTimeBinSampler(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        drop_last=False
    )

    # åˆ›å»º DataLoaderï¼ˆä½¿ç”¨ batch_samplerï¼‰
    train_loader = DataLoader(
        train_dataset,
        batch_sampler=train_sampler,
        num_workers=config['num_workers'],
        pin_memory=True if device.type == 'cuda' else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_sampler=val_sampler,
        num_workers=config['num_workers'],
        pin_memory=True if device.type == 'cuda' else False
    )

    print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)} | éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
    print(f"  ä½¿ç”¨æ—¶é—´åˆ†ç®±ç­–ç•¥ä¼˜åŒ– ConvLSTM å†…å­˜ä½¿ç”¨")

    # åˆ›å»ºæ‰¹æ¬¡å¤„ç†å™¨
    batch_processor = SlidingWindowBatchProcessor(sw_manager, tec_manager, device)

    # ==================== 4. åˆå§‹åŒ–æ¨¡å‹ ====================
    print("\n[æ­¥éª¤ 4] åˆå§‹åŒ– R-STMRF æ¨¡å‹...")

    model = R_STMRF_Model(
        iri_proxy=iri_proxy,
        lat_range=config['lat_range'],
        lon_range=config['lon_range'],
        alt_range=config['alt_range'],
        sw_manager=sw_manager,
        tec_manager=tec_manager,
        start_date_str=config['start_date_str'],
        config=config,
        tec_cache=None  # å…ˆä¸ä¼ ç¼“å­˜ï¼Œç­‰æ¨¡å‹åˆ›å»ºåå†åˆå§‹åŒ–
    ).to(device)

    print(f"  æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # ==================== 4.5. åˆå§‹åŒ–å°æ—¶çº§TECç¼“å­˜ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰====================
    # é‡è¦ï¼šå¿…é¡»åœ¨æ¨¡å‹åˆ›å»ºååˆå§‹åŒ–ï¼Œä»¥ä½¿ç”¨æ¨¡å‹å†…éƒ¨çš„ ConvLSTM å’Œæ¢¯åº¦å¤´
    use_tec_cache = config.get('use_tec_cache', False)  # é»˜è®¤å…³é—­ï¼Œä¿æŒå‘åå…¼å®¹

    if use_tec_cache:
        print("\n[æ­¥éª¤ 4.5] åˆå§‹åŒ–å°æ—¶çº§TECç¼“å­˜ï¼ˆå¤šæ—¶é—´å°ºåº¦ä¼˜åŒ–ï¼‰...")
        from .hourly_tec_cache import HourlyTECContextCache

        # ä½¿ç”¨æ¨¡å‹å†…éƒ¨çš„ ConvLSTM å’Œæ¢¯åº¦å¤´ï¼ˆå‚æ•°å…±äº«ï¼ï¼‰
        tec_cache = HourlyTECContextCache(
            convlstm_encoder=model.spatial_context_encoder,
            tec_gradient_head=model.tec_gradient_direction_head,
            max_cache_size=config.get('tec_cache_size', 100),
            device=device
        )

        # å°†ç¼“å­˜ç»‘å®šåˆ°æ¨¡å‹
        model.tec_cache = tec_cache

        print(f"  âœ“ TECç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼ˆæœ€å¤§ç¼“å­˜: {config.get('tec_cache_size', 100)} å°æ—¶ï¼‰")
        print(f"  âœ“ ConvLSTM å°†åªåœ¨æ•´ç‚¹å°æ—¶è¿è¡Œï¼Œä½¿ç”¨ä½™å¼¦å¹³æ–¹æ’å€¼åˆ°åˆ†é’Ÿçº§")
        print(f"  âœ“ ç¼“å­˜ä¸æ¨¡å‹å‚æ•°å…±äº«ï¼Œç¡®ä¿è®­ç»ƒæ—¶ä¸€è‡´æ€§")
        print(f"  âœ“ è®­ç»ƒæ¨¡å¼ï¼šè·³è¿‡ç¼“å­˜ï¼ˆä¿ç•™æ¢¯åº¦ï¼‰| è¯„ä¼°æ¨¡å¼ï¼šå¯ç”¨ç¼“å­˜ï¼ˆåŠ é€Ÿæ¨ç†ï¼‰")

    # ==================== 5. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ====================
    print("\n[æ­¥éª¤ 5] é…ç½®ä¼˜åŒ–å™¨...")

    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['lr'],
        weight_decay=config['weight_decay']
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config['scheduler_type'] == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config['epochs'],
            eta_min=config['min_lr']
        )
    else:
        scheduler = None

    # ==================== 6. è®­ç»ƒå¾ªç¯ ====================
    print("\n[æ­¥éª¤ 6] å¼€å§‹è®­ç»ƒ...")
    physics_freq = config.get('physics_loss_freq', 10)
    if physics_freq > 1:
        print(f"  âš¡ ç‰©ç†æŸå¤±é—´æ­‡æ€§è®¡ç®—ï¼šæ¯ {physics_freq} ä¸ªbatchè®¡ç®—ä¸€æ¬¡ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰")
        print(f"  âš¡ é¢„æœŸåŠ é€Ÿ: ~{physics_freq/2:.1f}Ã— æ¢¯åº¦è®¡ç®—å‡å°‘")
    else:
        print(f"  ğŸ“Š ç‰©ç†æŸå¤±æ¯ä¸ªbatchè®¡ç®—ï¼ˆphysics_loss_freq=1ï¼‰")
    print(f"{'='*70}\n")

    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(config['epochs']):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch+1}/{config['epochs']}")
        print(f"{'='*70}")

        # è®­ç»ƒ
        train_loss, train_dict = train_one_epoch(
            model, train_loader, batch_processor, optimizer, device, config, epoch, use_tec_cache
        )
        train_losses.append(train_loss)

        # éªŒè¯
        val_loss, val_metrics = validate(model, val_loader, batch_processor, device, config, use_tec_cache)
        val_losses.append(val_loss)

        # æ‰“å°ç»“æœ
        print(f"\nEpoch {epoch+1} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f}")
        print(f"    - MSE: {train_dict['mse']:.6f}")
        print(f"    - Physics: {train_dict['physics']:.6f}")
        print(f"      Â· Chapman: {train_dict['chapman']:.6f}")
        print(f"      Â· TEC Direction: {train_dict['tec_direction']:.6f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.6f}")
        print(f"    - MAE: {val_metrics['mae']:.6f}")
        print(f"    - RMSE: {val_metrics['rmse']:.6f}")
        print(f"    - RÂ²: {val_metrics['r2']:.4f}")

        # TECç¼“å­˜ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if use_tec_cache and tec_cache is not None:
            cache_stats = tec_cache.get_stats()
            print(f"  TECç¼“å­˜ç»Ÿè®¡:")
            print(f"    - å‘½ä¸­ç‡: {cache_stats['hit_rate']*100:.1f}% "
                  f"({cache_stats['hits']}/{cache_stats['total_queries']})")
            print(f"    - ç¼“å­˜å¤§å°: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")

        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None:
            scheduler.step()
            print(f"  å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            save_path = os.path.join(config['save_dir'], 'best_r_stmrf_model.pth')
            torch.save(model.state_dict(), save_path)
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹: {save_path}")
        else:
            patience_counter += 1

        # æ—©åœ
        if config['early_stopping'] and patience_counter >= config['patience']:
            print(f"\næ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±è¿ç»­ {config['patience']} è½®æœªæ”¹å–„")
            break

        # å®šæœŸä¿å­˜
        if (epoch + 1) % config['save_interval'] == 0:
            save_path = os.path.join(config['save_dir'], f'r_stmrf_epoch_{epoch+1}.pth')
            torch.save(model.state_dict(), save_path)

    print(f"\n{'='*70}")
    print("è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"{'='*70}\n")

    return model, train_losses, val_losses, train_loader, val_loader, sw_manager, tec_manager


# ======================== ä¸»å‡½æ•° ========================
if __name__ == '__main__':
    # è·å–é…ç½®
    config = get_config_r_stmrf()
    print_config_r_stmrf()

    # å¼€å§‹è®­ç»ƒ
    model, train_losses, val_losses, train_loader, val_loader, sw_manager, tec_manager = train_r_stmrf(config)

    print("\nè®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼")
